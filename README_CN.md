# MCP-ClickHouse Serverï¼ˆä¸­æ–‡ï¼‰

**ğŸ‘‰ [English Version](README.md)**  

è½»é‡çº§ [Model Context Protocol](https://modelcontextprotocol.io/)ï¼ˆMCPï¼‰æœåŠ¡ï¼Œä¸“ä¸º **å•å®ä¾‹ ClickHouse** è®¾è®¡ã€‚  
é€šè¿‡ MCP å·¥å…· / èµ„æºæš´éœ²æŸ¥è¯¢ã€å†™å…¥ä¸ç»“æ„å‘ç°èƒ½åŠ›ï¼Œè®© LLM ä»£ç†æˆ–å…¶ä»– MCP å®¢æˆ·ç«¯å¯ç”¨ç®€æ´çš„ JSON-RPC æ¥å£æ“ä½œ ClickHouse â€”â€” **æ— éœ€é©±åŠ¨ã€DSN æˆ–è§£æ SQLã€‚**

---

## æ ¸å¿ƒç‰¹æ€§
* ä»»æ„ SQL æŸ¥è¯¢ï¼Œç»“æœæ”¯æŒ JSON / CSV / Pretty
* é«˜ååæ‰¹é‡ `INSERT`
* å³æ—¶æ¶æ„ / æ•°æ®å‘ç°ï¼š`schema://â€¦` & `data://â€¦`
* **ğŸ¤– AI åŠ©æ‰‹ Prompts** â€”â€” 5ä¸ªä¸“ä¸šçš„ ClickHouse å’¨è¯¢åŠ©æ‰‹ï¼Œæ¶µç›–è¡¨åˆ†æã€æŸ¥è¯¢ä¼˜åŒ–ã€æ¨¡å¼è®¾è®¡ã€æ€§èƒ½æ•…éšœæ’æŸ¥å’Œè¿ç§»è§„åˆ’
* å†…ç½® Prometheus `/metrics`
* å¯é€‰ OpenTelemetryï¼ˆOTLPï¼‰é“¾è·¯è¿½è¸ª
* å®Œå…¨å¼‚æ­¥ï¼ˆFastAPI + clickhouse-driverï¼‰ï¼Œç”Ÿäº§å°±ç»ª
* æ”¯æŒ STDIO / SSE / Streamable-HTTPï¼Œå¤šç§éƒ¨ç½²æ–¹å¼

---

## å¯ç”¨å·¥å…·

| å·¥å…· | åˆ†ç±» | è¯´æ˜ |
|------|------|------|
| `query`            | SQL    | æ‰§è¡Œä»»æ„ SQL å¹¶æµå¼è¿”å›ç»“æœ |
| `insert`           | SQL    | æ‰¹é‡å†™å…¥è¡Œï¼ˆ`[{åˆ—: å€¼,â€¦}]`ï¼‰ |
| `create_database`  | Schema | æ–°å»ºæ•°æ®åº“ |
| `create_table`     | Schema | åˆ›å»ºè¡¨ï¼ˆå¼•æ“ / åˆ—ä¿¡æ¯ JSONï¼‰ |
| `server_info`      | Admin  | è¿”å› ClickHouse ç‰ˆæœ¬ã€è¿è¡Œæ—¶é—´ã€æ•°æ®åº“æ•°é‡ç­‰ |

_èµ„æº_
`resource://schema` â€”â€” æµè§ˆæ•°æ®åº“ / è¡¨ / åˆ—å¹¶è·å– DDL
`resource://data`   â€”â€” è·å–ç¤ºä¾‹è¡Œã€è®¡æ•°æˆ–é€šè¿‡æŸ¥è¯¢å‚æ•°æ‰§è¡Œåªè¯» SQL

## AI åŠ©æ‰‹ Prompts

| Prompt | åŠŸèƒ½ | è¯´æ˜ |
|--------|------|------|
| `analyze_table`              | è¡¨åˆ†æ | åˆ†æ ClickHouse è¡¨ç»“æ„ã€æ•°æ®åˆ†å¸ƒå¹¶å»ºè®®ä¼˜åŒ–æ–¹æ¡ˆ |
| `optimize_query`             | æŸ¥è¯¢ä¼˜åŒ– | åˆ†æå¹¶å»ºè®® ClickHouse SQL æŸ¥è¯¢çš„ä¼˜åŒ–æ–¹æ¡ˆ |
| `design_schema`              | æ¨¡å¼è®¾è®¡ | ä¸ºç‰¹å®šç”¨ä¾‹è®¾è®¡æœ€ä¼˜çš„ ClickHouse æ•°æ®åº“æ¨¡å¼ |
| `troubleshoot_performance`   | æ€§èƒ½æ•…éšœæ’æŸ¥ | è¯Šæ–­å’Œè§£å†³ ClickHouse æ€§èƒ½é—®é¢˜ |
| `plan_migration`             | è¿ç§»è§„åˆ’ | ä¸ºä»æŒ‡å®šæºç³»ç»Ÿåˆ° ClickHouse çš„æ•°æ®è¿ç§»ç”Ÿæˆè¯¦ç»†è§„åˆ’æ–¹æ¡ˆ |

_æ¯ä¸ª Prompt éƒ½æä¾›ä¸­è‹±æ–‡ç‰ˆæœ¬ï¼Œå¸®åŠ©ç”¨æˆ·è·å¾—ä¸“ä¸šçš„ ClickHouse å’¨è¯¢å»ºè®®_

---

## å¿«é€Ÿå¼€å§‹

### 1 Â· Docker

```bash
docker run --rm -p 8000:8000 \
  -e CH_HOST=host.docker.internal \
  mcp/clickhouse run -t sse
```

### 2 Â· Poetry æœ¬åœ°è¿è¡Œ

```bash
poetry install
poetry run mcp-clickhouse run -t sse
```

---

## é…ç½®

æ‰€æœ‰é€‰é¡¹å‡ä¸ºç¯å¢ƒå˜é‡ï¼ˆå¯æ”¾å…¥ `.env`ï¼‰ã€‚

> æœ¬é¡¹ç›®ç›´æ¥ä½¿ç”¨ [FastMCP](https://github.com/modelcontextprotocol/python-sdk) çš„åŸç”Ÿé…ç½®ç³»ç»Ÿã€‚  
> ä½¿ç”¨ `FASTMCP_` å‰ç¼€çš„ç¯å¢ƒå˜é‡ç›´æ¥é…ç½® FastMCP æœåŠ¡å™¨ï¼Œæ— éœ€é¢å¤–çš„å°è£…å±‚ã€‚  
> è¿™æ ·è®¾è®¡æ›´ç®€å•ã€æ›´ç›´æ¥ï¼Œå‡å°‘äº†é…ç½®å¤æ‚æ€§ã€‚

FastMCP æœåŠ¡å™¨é…ç½®ç¤ºä¾‹ï¼š

```dotenv
# FastMCP æœåŠ¡å™¨åŸºæœ¬è®¾ç½®
FASTMCP_HOST=0.0.0.0
FASTMCP_PORT=3000
FASTMCP_DEBUG=false
FASTMCP_LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

# FastMCP è·¯å¾„é…ç½®
FASTMCP_MOUNT_PATH=/mcp
FASTMCP_SSE_PATH=/sse
FASTMCP_MESSAGE_PATH=/messages/
FASTMCP_STREAMABLE_HTTP_PATH=/mcp

# FastMCP ä¼ è¾“è®¾ç½®
FASTMCP_STATELESS_HTTP=false
FASTMCP_JSON_RESPONSE=false

# FastMCP èµ„æºç®¡ç†
FASTMCP_WARN_ON_DUPLICATE_RESOURCES=true
FASTMCP_WARN_ON_DUPLICATE_TOOLS=true
FASTMCP_WARN_ON_DUPLICATE_PROMPTS=true
```

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| **MCP** |||
| `FASTMCP_HOST` | `0.0.0.0` | ç›‘å¬åœ°å€ |
| `FASTMCP_PORT` | `3000` | ç›‘å¬ç«¯å£ |
| `FASTMCP_TRANSPORT` | `streamable-http` | `streamable-http` / `sse` / `stdio` |
| **ClickHouse** |||
| `CH_HOST` | `localhost` | ä¸»æœº |
| `CH_PORT` | `9000` | ç«¯å£ |
| `CH_USER` / `CH_PASSWORD` | `default` / ç©º | å‡­æ® |
| `CH_DATABASE` | `default` | é»˜è®¤æ•°æ®åº“ |
| `CH_SECURE` | `false` | æ˜¯å¦å¯ç”¨ TLS |
| **å¯è§‚æµ‹æ€§** |||
| `LOG_LEVEL` | `INFO` | æ—¥å¿—çº§åˆ« |
| `METRICS_ENABLED` | `true` | æš´éœ² `/metrics` |
| `TRACING_ENABLED` | `false` | å¯ç”¨ OTLP |
| `OTLP_ENDPOINT` |  | Collector URL |
| **å…¶ä»–** |||
| `TEMP_DIR` | `/tmp/mcp-clickhouse` | ä¸´æ—¶ç›®å½• |
| `MAX_UPLOAD_SIZE` | `104857600` | ä¸Šä¼ å¤§å°ä¸Šé™ï¼ˆå­—èŠ‚ï¼‰ |

---

## ä½¿ç”¨ä¸é›†æˆ

### STDIOï¼ˆæœ¬åœ° AI åŠ©æ‰‹æœ€ç®€ï¼‰

```bash
poetry run mcp-clickhouse run -t stdio
```

### SSE / Streamable-HTTP æœåŠ¡æ¨¡å¼

```bash
# SSE
poetry run mcp-clickhouse run -t sse
# Streamable-HTTP
poetry run mcp-clickhouse run -t streamable-http
```

### Claude Desktop é…ç½®ç¤ºä¾‹

```jsonc
{
  "mcpServers": {
    "clickhouse": {
      "command": "mcp-clickhouse",
      "args": ["-t", "stdio"],
      "env": {
        "CH_HOST": "localhost",
        "CH_PORT": "9000"
      }
    }
  }
}
```

> è‹¥ä½¿ç”¨ Dockerï¼Œå¯å°† `command` æ›¿æ¢ä¸º  
> `docker run --rm -i mcp/clickhouse run -t stdio`

### Claude Desktopï¼ˆSSEæ¨¡å¼ï¼‰

å¦‚æœä½ å¸Œæœ›ä½¿ç”¨ **SSE æ¨¡å¼**ï¼ˆHTTP é•¿è½®è¯¢ï¼Œå¯æ”¯æŒå¤šå®¢æˆ·ç«¯å¹¶å‘ï¼‰è¿è¡ŒæœåŠ¡å™¨ï¼Œå¯æŒ‰ä»¥ä¸‹æ–¹å¼é…ç½® Claude Desktopï¼š

```jsonc
{
  "mcpServers": {
    "clickhouse": {
      "type": "sse",
      "url": "http://localhost:8000/sse"
    }
  }
}
```

åœ¨ä½¿ç”¨ä¸Šè¿°é…ç½®å‰ï¼Œéœ€è¦å…ˆå•ç‹¬ä»¥ **SSE æ¨¡å¼**å¯åŠ¨æœåŠ¡å™¨ï¼š

```bash
# æœ¬åœ°ï¼ˆPoetryï¼‰
poetry run mcp-clickhouse run -t sse

# æˆ–ä½¿ç”¨ Docker
docker run --rm -p 8000:8000 \
  -e CH_HOST=host.docker.internal \
  mcp/clickhouse run -t sse
```

---

## ä½¿ç”¨ç¤ºä¾‹

æœ¬èŠ‚æä¾›å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼Œä»åŸºç¡€æ“ä½œåˆ°AIåŠ©æ‰‹åŠŸèƒ½ï¼Œå¸®åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹å¹¶å……åˆ†åˆ©ç”¨MCP-ClickHouseçš„æ‰€æœ‰åŠŸèƒ½ã€‚

### åŸºç¡€æ“ä½œç¤ºä¾‹

#### å¿«é€ŸæŸ¥è¯¢ - ç³»ç»Ÿä¿¡æ¯å’ŒåŸºç¡€æ“ä½œ

```python
from mcp.client.quick import call_tool

# è·å–ClickHouseæœåŠ¡å™¨ä¿¡æ¯
server_info = call_tool(
    "http://localhost:3000/mcp",
    "server_info",
    {}
)
print("=== æœåŠ¡å™¨ä¿¡æ¯ ===")
print(f"ç‰ˆæœ¬: {server_info.get('version')}")
print(f"è¿è¡Œæ—¶é—´: {server_info.get('uptime')}")

# æ‰§è¡ŒåŸºç¡€æŸ¥è¯¢
current_time = call_tool(
    "http://localhost:3000/mcp",
    "query",
    {
        "sql": "SELECT now() as current_time, version() as version, uptime() as uptime",
        "format": "JSONEachRow"
    }
)
print("\n=== å½“å‰çŠ¶æ€ ===")
for row in current_time:
    print(f"æ—¶é—´: {row[0]}")
    print(f"ç‰ˆæœ¬: {row[1]}")
    print(f"è¿è¡Œæ—¶é—´: {row[2]}ç§’")

# æŸ¥çœ‹æ•°æ®åº“åˆ—è¡¨
databases = call_tool(
    "http://localhost:3000/mcp",
    "query",
    {"sql": "SHOW DATABASES"}
)
print(f"\n=== å¯ç”¨æ•°æ®åº“ ({len(databases)}ä¸ª) ===")
for db in databases:
    print(f"- {db[0]}")
```

### é«˜çº§æ“ä½œç¤ºä¾‹

#### å®Œæ•´æ•°æ®å·¥ä½œæµç¨‹ - åˆ›å»ºã€æ’å…¥ã€æŸ¥è¯¢ã€åˆ†æ

```python
import asyncio
import random
import datetime as dt
from mcp.client.session import ClientSession
from mcp.client.streamable_http import streamablehttp_client

async def complete_data_workflow():
    """æ¼”ç¤ºå®Œæ•´çš„æ•°æ®æ“ä½œæµç¨‹"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            # 1. åˆ›å»ºæ•°æ®åº“
            print("=== 1. åˆ›å»ºæµ‹è¯•æ•°æ®åº“ ===")
            await session.call_tool("create_database", {
                "database": "demo_analytics"
            })
            
            # 2. åˆ›å»ºè¡¨
            print("=== 2. åˆ›å»ºç”¨æˆ·äº‹ä»¶è¡¨ ===")
            await session.call_tool("create_table", {
                "database": "demo_analytics",
                "table": "user_events",
                "engine": "MergeTree",
                "columns": [
                    {"name": "event_time", "type": "DateTime"},
                    {"name": "user_id", "type": "UInt32"},
                    {"name": "event_type", "type": "String"},
                    {"name": "page_url", "type": "String"},
                    {"name": "session_duration", "type": "UInt32"}
                ],
                "order_by": ["event_time", "user_id"],
                "partition_by": "toYYYYMM(event_time)"
            })
            
            # 3. ç”Ÿæˆå¹¶æ’å…¥æµ‹è¯•æ•°æ®
            print("=== 3. æ’å…¥æµ‹è¯•æ•°æ® ===")
            events = []
            event_types = ['login', 'view', 'click', 'purchase', 'logout']
            pages = ['/home', '/product', '/cart', '/checkout', '/profile']
            
            for i in range(1000):
                events.append({
                    "event_time": (dt.datetime.now() - dt.timedelta(
                        minutes=random.randint(0, 1440)
                    )).strftime('%Y-%m-%d %H:%M:%S'),
                    "user_id": random.randint(1, 100),
                    "event_type": random.choice(event_types),
                    "page_url": random.choice(pages),
                    "session_duration": random.randint(10, 3600)
                })
            
            await session.call_tool("insert", {
                "database": "demo_analytics",
                "table": "user_events",
                "rows": events
            })
            print(f"æˆåŠŸæ’å…¥ {len(events)} æ¡è®°å½•")
            
            # 4. æ•°æ®éªŒè¯å’ŒåŸºç¡€åˆ†æ
            print("=== 4. æ•°æ®éªŒè¯å’Œåˆ†æ ===")
            
            # æ€»è®°å½•æ•°
            total_count = await session.call_tool("query", {
                "sql": "SELECT count() FROM demo_analytics.user_events"
            })
            print(f"æ€»è®°å½•æ•°: {total_count[0][0]:,}")
            
            # ç”¨æˆ·æ´»è·ƒåº¦åˆ†æ
            user_activity = await session.call_tool("query", {
                "sql": """
                SELECT
                    event_type,
                    count() as event_count,
                    uniq(user_id) as unique_users,
                    avg(session_duration) as avg_duration
                FROM demo_analytics.user_events
                GROUP BY event_type
                ORDER BY event_count DESC
                """
            })
            
            print("\nç”¨æˆ·æ´»è·ƒåº¦åˆ†æ:")
            print("äº‹ä»¶ç±»å‹\t\täº‹ä»¶æ•°\t\tç‹¬ç«‹ç”¨æˆ·\tå¹³å‡æ—¶é•¿")
            print("-" * 60)
            for row in user_activity:
                print(f"{row[0]:<12}\t{row[1]:<8}\t{row[2]:<8}\t{row[3]:.1f}ç§’")
            
            return session, "demo_analytics", "user_events"

# è¿è¡ŒåŸºç¡€å·¥ä½œæµç¨‹
session, database, table = asyncio.run(complete_data_workflow())
```

#### èµ„æºå‘ç°å’Œç³»ç»Ÿç›‘æ§

```python
async def advanced_features_demo():
    """æ¼”ç¤ºé«˜çº§åŠŸèƒ½ï¼šèµ„æºå‘ç°ã€ç›‘æ§å’Œä¼˜åŒ–"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            # 1. ä½¿ç”¨èµ„æºå‘ç°åŠŸèƒ½
            print("=== 1. æ•°æ®åº“æ¶æ„å‘ç° ===")
            
            # è·å–æ‰€æœ‰æ•°æ®åº“çš„æ¶æ„ä¿¡æ¯
            schema_info = await session.get_resource("resource://schema")
            print("å¯ç”¨æ•°æ®åº“æ¶æ„:")
            print(schema_info.get("content", ""))
            # è·å–è¡¨æ ·æœ¬æ•°æ®ï¼ˆæ”¯æŒæ—¶é—´è¿‡æ»¤ï¼‰
            sample = await session.get_resource(
                "resource://data/demo_analytics/user_events/sample"
                "?start_time=2024-07-01T00:00:00"
                "&end_time=2024-07-01T23:59:59"
                "&time_column=event_time"
                "&limit=5"
            )
            print("è¡¨æ ·æœ¬æ•°æ®ï¼ˆæŒ‡å®šæ—¶é—´èŒƒå›´ï¼‰:")
            print(sample)
            
            # 2. ç³»ç»Ÿæ€§èƒ½ç›‘æ§
            print("\n=== 2. ç³»ç»Ÿæ€§èƒ½ç›‘æ§ ===")
            
            # æŸ¥è¯¢å½“å‰ç³»ç»Ÿè´Ÿè½½
            system_metrics = await session.call_tool("query", {
                "sql": """
                SELECT
                    'CPUæ ¸å¿ƒæ•°' as metric,
                    toString(value) as value
                FROM system.asynchronous_metrics
                WHERE metric = 'jemalloc.background_thread.num_threads'
                UNION ALL
                SELECT
                    'å†…å­˜ä½¿ç”¨é‡',
                    formatReadableSize(value)
                FROM system.asynchronous_metrics
                WHERE metric = 'MemoryTracking'
                """
            })
            
            print("ç³»ç»Ÿèµ„æºçŠ¶æ€:")
            for metric in system_metrics:
                print(f"- {metric[0]}: {metric[1]}")

asyncio.run(advanced_features_demo())
```

### AI åŠ©æ‰‹åŠŸèƒ½ç¤ºä¾‹

åŸºäºä¸Šè¿°åŸºç¡€æ“ä½œï¼Œæ‚¨å¯ä»¥ä½¿ç”¨AIåŠ©æ‰‹è·å¾—ä¸“ä¸šçš„ClickHouseå’¨è¯¢å»ºè®®ï¼š

#### è¡¨åˆ†æ - æ·±åº¦åˆ†æè¡¨ç»“æ„å’Œæ€§èƒ½
```python
import asyncio
from mcp.client.session import ClientSession
from mcp.client.streamable_http import streamablehttp_client

async def analyze_table_performance():
    """åˆ†æè¡¨ç»“æ„ã€æ•°æ®åˆ†å¸ƒå’Œæ€§èƒ½ç“¶é¢ˆ"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            # è·å–ä¸“ä¸šçš„è¡¨åˆ†æå»ºè®®
            result = await session.call_tool("prompts/get", {
                "name": "analyze_table",
                "arguments": {
                    "database": "ecommerce",
                    "table": "order_events",
                    "sample_size": "10000"  # åˆ†æ1ä¸‡æ¡æ ·æœ¬æ•°æ®
                }
            })
            
            print("=== è¡¨åˆ†ææŠ¥å‘Š ===")
            print(result[0]["content"]["text"])
            
            # å¯ä»¥ç»“åˆå®é™…æŸ¥è¯¢éªŒè¯å»ºè®®
            stats = await session.call_tool("query", {
                "sql": "SELECT count(), formatReadableSize(sum(data_compressed_bytes)) FROM system.parts WHERE database='ecommerce' AND table='order_events'"
            })
            print(f"å½“å‰è¡¨ç»Ÿè®¡: {stats}")

# è¿è¡Œåˆ†æ
asyncio.run(analyze_table_performance())
```

### æŸ¥è¯¢ä¼˜åŒ– - æ™ºèƒ½SQLæ€§èƒ½è°ƒä¼˜
```python
async def optimize_slow_query():
    """è·å–æŸ¥è¯¢ä¼˜åŒ–å»ºè®®å¹¶éªŒè¯æ•ˆæœ"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            slow_query = """
            SELECT
                user_id,
                COUNT(*) as event_count,
                AVG(session_duration) as avg_duration
            FROM user_events
            WHERE event_date >= '2024-01-01'
                AND event_type IN ('login', 'purchase', 'view')
            GROUP BY user_id
            HAVING COUNT(*) > 10
            ORDER BY event_count DESC
            LIMIT 100
            """
            
            # è·å–ä¼˜åŒ–å»ºè®®
            optimization = await session.call_tool("prompts/get", {
                "name": "optimize_query",
                "arguments": {
                    "query": slow_query,
                    "context": "ç”¨æˆ·è¡Œä¸ºåˆ†ææŠ¥è¡¨ï¼Œæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼Œå½“å‰æ‰§è¡Œæ—¶é—´è¶…è¿‡30ç§’"
                }
            })
            
            print("=== æŸ¥è¯¢ä¼˜åŒ–å»ºè®® ===")
            print(optimization[0]["content"]["text"])
            
            # å¯ä»¥æµ‹è¯•åŸå§‹æŸ¥è¯¢æ€§èƒ½
            print("\n=== æ‰§è¡ŒåŸå§‹æŸ¥è¯¢ ===")
            import time
            start_time = time.time()
            result = await session.call_tool("query", {"sql": slow_query})
            execution_time = time.time() - start_time
            print(f"æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’, ç»“æœè¡Œæ•°: {len(result)}")

asyncio.run(optimize_slow_query())
```

### æ¨¡å¼è®¾è®¡ - ä¸ºä¸šåŠ¡åœºæ™¯è®¾è®¡æœ€ä¼˜æ¶æ„
```python
async def design_analytics_schema():
    """ä¸ºç‰¹å®šä¸šåŠ¡åœºæ™¯è®¾è®¡æ•°æ®åº“æ¨¡å¼"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            # è·å–æ¨¡å¼è®¾è®¡å»ºè®®
            schema_design = await session.call_tool("prompts/get", {
                "name": "design_schema",
                "arguments": {
                    "use_case": "ç”µå•†å¹³å°å®æ—¶æ•°æ®åˆ†æç³»ç»Ÿï¼Œéœ€è¦æ”¯æŒç”¨æˆ·è¡Œä¸ºè¿½è¸ªã€è®¢å•åˆ†æã€å•†å“æ¨èå’Œå®æ—¶å¤§å±å±•ç¤º",
                    "data_volume": "æ—¥æ´»ç”¨æˆ·100ä¸‡ï¼Œæ¯å¤©äº§ç”Ÿ5000ä¸‡äº‹ä»¶ï¼Œæ¯æœˆæ•°æ®å¢é•¿çº¦200GB",
                    "query_patterns": "å®æ—¶ç”¨æˆ·ç”»åƒæŸ¥è¯¢ã€å•†å“é”€å”®è¶‹åŠ¿åˆ†æã€æ¼æ–—è½¬åŒ–åˆ†æã€å®æ—¶ç›‘æ§å¤§å±èšåˆæŸ¥è¯¢"
                }
            })
            
            print("=== æ•°æ®åº“æ¨¡å¼è®¾è®¡æ–¹æ¡ˆ ===")
            print(schema_design[0]["content"]["text"])
            
            # å¯ä»¥æ ¹æ®å»ºè®®åˆ›å»ºç¤ºä¾‹è¡¨
            print("\n=== æ ¹æ®å»ºè®®åˆ›å»ºç¤ºä¾‹è¡¨ç»“æ„ ===")
            # è¿™é‡Œå¯ä»¥æ‰§è¡Œå»ºè®®çš„CREATE TABLEè¯­å¥

asyncio.run(design_analytics_schema())
```

### æ€§èƒ½æ•…éšœæ’æŸ¥ - ç³»ç»Ÿæ€§èƒ½é—®é¢˜è¯Šæ–­
```python
async def diagnose_performance_issues():
    """è¯Šæ–­å’Œè§£å†³ClickHouseæ€§èƒ½é—®é¢˜"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            # å…ˆæ”¶é›†ç³»ç»Ÿä¿¡æ¯
            system_info = await session.call_tool("query", {
                "sql": """
                SELECT
                    'CPUä½¿ç”¨ç‡' as metric,
                    toString(round(avg(ProfileEvent_OSCPUVirtualTimeMicroseconds)/1000000, 2)) as value
                FROM system.metric_log
                WHERE event_time >= now() - INTERVAL 1 HOUR
                UNION ALL
                SELECT
                    'å†…å­˜ä½¿ç”¨ç‡',
                    formatReadableSize(max(CurrentMetric_MemoryTracking))
                FROM system.metric_log
                WHERE event_time >= now() - INTERVAL 1 HOUR
                """
            })
            
            # è·å–æ€§èƒ½è¯Šæ–­å»ºè®®
            troubleshooting = await session.call_tool("prompts/get", {
                "name": "troubleshoot_performance",
                "arguments": {
                    "issue_description": "å¤§è¡¨èšåˆæŸ¥è¯¢å“åº”æ—¶é—´ä»5ç§’å¢é•¿åˆ°60ç§’ï¼Œå†…å­˜ä½¿ç”¨ç‡æŒç»­ä¸Šå‡ï¼Œå¶å°”å‡ºç°æŸ¥è¯¢è¶…æ—¶",
                    "slow_query": """
                    SELECT
                        toYYYYMM(order_date) as month,
                        category,
                        sum(amount) as total_sales,
                        count() as order_count
                    FROM orders
                    WHERE order_date >= '2023-01-01'
                    GROUP BY month, category
                    ORDER BY month DESC, total_sales DESC
                    """
                }
            })
            
            print("=== ç³»ç»Ÿå½“å‰çŠ¶æ€ ===")
            for row in system_info:
                print(f"{row[0]}: {row[1]}")
                
            print("\n=== æ€§èƒ½é—®é¢˜è¯Šæ–­æŠ¥å‘Š ===")
            print(troubleshooting[0]["content"]["text"])

asyncio.run(diagnose_performance_issues())
```

### è¿ç§»è§„åˆ’ - æ•°æ®åº“è¿ç§»å…¨æµç¨‹è§„åˆ’
```python
async def plan_database_migration():
    """åˆ¶å®šä»å…¶ä»–æ•°æ®åº“åˆ°ClickHouseçš„è¿ç§»æ–¹æ¡ˆ"""
    async with streamablehttp_client("http://localhost:3000/mcp") as (r, w, _):
        async with ClientSession(r, w) as session:
            await session.initialize()
            
            # è·å–è¿ç§»è§„åˆ’æ–¹æ¡ˆ
            migration_plan = await session.call_tool("prompts/get", {
                "name": "plan_migration",
                "arguments": {
                    "source_system": "MySQL 8.0 é›†ç¾¤ï¼ŒåŒ…å«è®¢å•ç³»ç»Ÿ(50GB)ã€ç”¨æˆ·ç³»ç»Ÿ(20GB)ã€æ—¥å¿—ç³»ç»Ÿ(500GB)",
                    "data_size": "æ€»è®¡570GBå†å²æ•°æ®ï¼Œæ¯å¤©æ–°å¢2GBï¼Œå³°å€¼QPS 5000",
                    "requirements": "ä¸šåŠ¡é›¶åœæœºè¿ç§»ï¼Œä¿æŒæ•°æ®å¼ºä¸€è‡´æ€§ï¼Œè¿ç§»åæŸ¥è¯¢æ€§èƒ½æå‡10å€ï¼Œæ”¯æŒå®æ—¶åˆ†æ"
                }
            })
            
            print("=== æ•°æ®åº“è¿ç§»è§„åˆ’æ–¹æ¡ˆ ===")
            print(migration_plan[0]["content"]["text"])
            
            # å¯ä»¥éªŒè¯æºç³»ç»Ÿè¿æ¥å’Œç›®æ ‡ç³»ç»Ÿå‡†å¤‡æƒ…å†µ
            print("\n=== è¿ç§»å‰ç¯å¢ƒæ£€æŸ¥ ===")
            ch_version = await session.call_tool("query", {
                "sql": "SELECT version()"
            })
            print(f"ç›®æ ‡ClickHouseç‰ˆæœ¬: {ch_version[0][0]}")
            
            # æ£€æŸ¥å¯ç”¨å­˜å‚¨ç©ºé—´
            storage_info = await session.call_tool("query", {
                "sql": """
                SELECT
                    formatReadableSize(free_space) as free_space,
                    formatReadableSize(total_space) as total_space
                FROM system.disks
                WHERE name = 'default'
                """
            })
            print(f"å¯ç”¨å­˜å‚¨ç©ºé—´: {storage_info[0][0]} / {storage_info[0][1]}")

asyncio.run(plan_database_migration())
```


---

## å¼€å‘æŒ‡å—

```bash
# å®‰è£…ä¾èµ–
poetry install

# ä»£ç æ ¼å¼ & ç±»å‹æ£€æŸ¥
ruff format .
ruff check . --fix

# è¿è¡Œæµ‹è¯•
poetry run pytest -v
```

### å‘å¸ƒé•œåƒ

```bash
docker build -t mcp/clickhouse .
docker run --rm -p 3000:3000 mcp/clickhouse
```

---

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue / PRï¼  
è¯·ç¡®ä¿ `pytest`ã€`black`ã€`isort`ã€`mypy` é€šè¿‡åå†æäº¤ã€‚

---

## è®¸å¯è¯

Apache 2.0 Â© 2025 San Francisco AI Factory
